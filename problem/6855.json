{"title":"Auto-correction","time_limit":5000,"memory_limit":512,"operation":{"submit":"http://acm.hdu.edu.cn/submit.php?pid=6855","submissions":"http://acm.hdu.edu.cn/status.php?first=&pid=6855&user=&lang=0&status=0","statistics":"http://acm.hdu.edu.cn/statistic.php?pid=6855","discussion":"http://acm.hdu.edu.cn/discuss/problem/list.php?problemid=6855"},"statement":[{"title":"Problem Description","content":"<b>It is preferrable to read the pdf statment.</b><br><br>Cuber QQ is poor in English writing, and in the process of preparing this contest, he realized that he is making too many grammar mistakes that an auto-correction engine is needed. Instead of using online tools like &apos;&apos;Microsoft Aim Writing&apos;&apos; or &apos;&apos;Grammarly&apos;&apos;, he was interested in building a new engine on his own.<br><br>In particular, he adopted a naive sequence-to-sequence model, that takes a sequence, which is usually a sentence, and predict for each token, which is usually a word or character, whether there is something wrong with it, and if yes, what it should be replaced with. Here are several examples:<br><br><ul><br><li> In &apos;&apos;Cuber QQ was one of the admirers Quber CC.&apos;&apos;, &apos;&apos;admirers&apos;&apos; should be replaced with &apos;&apos;admirers of&apos;&apos;.</li><br><li> In &apos;&apos;Cuber QQ confess his love to Cuber QQ just now.&apos;&apos;, &apos;&apos;confess&apos;&apos; should be replaced with &apos;&apos;confessed&apos;&apos;.</li><br><li> In &apos;&apos;Quber CC said that they are being and always will be good friends.&apos;&apos;, &apos;&apos;are being&apos;&apos; should be replaced with &apos;&apos;are&apos;&apos;.</li><br></ul><br><br>You might notice that, in this sequence-to-sequence model, the phrase to replace should be at least one token, and the target should be at least one token too. This is related to the architecture and training approach of his model. We will not go into too many machine learning details here, as it will make the statement tedious. The problem is however, the training data does not conform with such format. In the training data, a sequence with flaws can be annotated with three types of annotations: add, delete and replace. Concretely,<br><br><ul><br><li> $A$ $l$ $s_1$ $s_2$ $\\cdots$ $s_v$: to add sequence $s$ before position $l$.</li><br><li> $D$ $l$ $r$: to delete from $l$-th token to the $r$-th token, inclusive.</li><br><li> $R$ $l$ $r$ $s_1$ $s_2$ $\\cdots$ $s_v$: to replace sub-sequence from $l$-th token to $r$-th token, inclusive, with sequence $s$.</li><br></ul><br><br>All the annotations are applied directly to the original sequence, i.e., the indices like $l$ and $r$ refers to the original indices, instead of the indices after modification.<br><br>As &apos;&apos;add&apos;&apos; and &apos;&apos;delete&apos;&apos; will not be supported in the model, the preprocessing step needs to rewrite all &apos;&apos;add&apos;&apos; and &apos;&apos;delete&apos;&apos; with &apos;&apos;replace&apos;&apos;. Furthermore, as there are many ways to achieve such goal, Cuber QQ wants to find the cheapest way, i.e., after the annotation rewriting, the total number of replaced tokens should be as minimum as possible. If there is a tie, the number of annotation records should be as minimum as possible. In case there is still a tie, any one of them is acceptable.","type":"description","format":"html","require":["katex"]},{"title":"Input","content":"The input starts with an integer $t$ ($1 \\le T \\le 50~000$), denoting the number of test cases.<br><br>For each test case, the first line contains two space-separated integers $n$ and $q$ ($1 \\le n, q \\le 2~000$), where $n$ is the number of tokens in the original sequence, and $q$ is the number of original annotations.<br><br>In the next line, $n$ integers $a_1, a_2, \\ldots, a_n$ ($1 \\le a_i \\le n$) are presented, denoting the sequence.<br><br>The $i$-th of the following $q$ lines is in one of the 3 formats:<br><br><ul><br><li> $A$ $l_i$ $s_{i,1}$ $s_{i,2}$ $\\cdots$ $s_{i,{v_i}}$ ($1 \\le l_i \\le n + 1$, $1 \\le s_{i,k} \\le n$). Notably, when $l_i = n + 1$, it is to add tokens at the end of sequence.</li><br><li> $D$ $l_i$ $r_i$ ($1 \\le l_i \\le r_i \\le n$).</li><br><li> $R$ $l_i$ $r_i$ $s_{i,1}$ $s_{i,2}$ $\\cdots$ $s_{i,{v_i}}$ ($1 \\le l_i \\le r_i \\le n$, $1 \\le s_{i,k} \\le n$).</li><br></ul><br><br>It is guaranteed that $l_i \\le l_{i+1}$ for all $1 \\le i &lt; n$, and $l_i = l_{i+1}$ only happens when $i$ is $A$ and $i+1$ is not, which means, there is at most one &apos;&apos;add&apos;&apos; at the same position. The annotations are non-overlapping, i.e., $r_i \\le l_{i+1}$ for all $1 \\le i &lt; n$ if $r_i$ is available for $i$. Furthermore, the corrected sequence after applying all the annotations is not empty.<br><br>It is guaranteed that for each test case, the corrected sequence is neither empty nor longer than $4~000$. The sum of $n$ and the total length of corrected sequences both do not exceed $50~000$.","type":"input_format","format":"html","require":["katex"]},{"title":"Output","content":"For each test case, in the first line output two space-separated integers: minimum number of tokens that will be replaced, $x$, and minimum number of converted annotations, $y$.<br><br>In the following $y$ lines, you can output the annotations in any order. $R$ should be omitted as it is the only type that is allowed. The annotations should be non-overlapping, non-empty and follow the exactly same format as input.<br><br><h3>Note</h3><br>For the first test case, $[2, 3]$ is replaced with $1$, $[4, 6]$ is replaced with $2,3$, and the corrected sequence is $1,1,2,3$. The optimal correction with only $R$ is to replace $[2, 6]$ with $1,2,3$.<br><br>For the second test case, the corrected sequence is $1,2,4,6,5$. Although $A$ and $D$ cannot be used, if we merge the consecutive annotations, only 4 tokens need to be replaced.<br><br>In the third test case, we show that the corrected sequence can be longer than the original sequence, which is $2,1,2,3,4,5,5,6$.","type":"output_format","format":"html","require":["katex"]},{"title":"Sample Input","content":"<pre><div style=\"font-family:Courier New,Courier,monospace;\">3\n6 2\n1 2 5 3 4 6\nR 2 3 1\nR 4 6 2 3\n6 3\n1 2 2 3 4 6\nR 3 4 4\nD 5 5\nA 7 5\n6 2\n1 2 3 4 5 6\nA 1 2\nA 6 5</div></pre>","type":"example","format":"html","require":["katex"]},{"title":"Sample Output","content":"<pre><div style=\"font-family:Courier New,Courier,monospace;\">5 1\n2 6 1 2 3\n4 1\n3 6 4 6 5\n2 2\n6 6 5 6\n1 1 2 1</div></pre>","type":"example","format":"html","require":["katex"]},{"title":"Source","content":" <a href=\"/search.php?field=problem&amp;key=2020+Multi-University+Training+Contest+8&amp;source=1&amp;searchmode=source\"> 2020 Multi-University Training Contest 8 </a> ","type":"source","format":"html","require":["katex"]}]}